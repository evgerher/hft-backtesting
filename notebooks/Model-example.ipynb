{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\cygwin64\\home\\evger\\thesis\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from collections import deque, namedtuple, defaultdict\n",
    "from typing import Union, List, Dict, Tuple, NamedTuple, Deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hft.utils import logger\n",
    "logger.to_file = True\n",
    "logger.fmt_string = \"%(message)s\"\n",
    "\n",
    "from hft.backtesting.backtest import BacktestOnSample\n",
    "from hft.backtesting.data import OrderStatus, OrderRequest\n",
    "from hft.backtesting.readers import OrderbookReader\n",
    "from hft.backtesting.strategy import Strategy\n",
    "from hft.units.metrics.composite import Lipton\n",
    "from hft.units.metrics.instant import VWAP_volume, LiquiditySpectrum, HayashiYoshido\n",
    "from hft.units.metrics.time import TradeMetric\n",
    "from hft.utils.data import Trade, OrderBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN \n",
    "\n",
    "Dueling DQN implementation with target network implementation on pytorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "  def __init__(self, input_dim: int, output_dim: int):\n",
    "    super().__init__()\n",
    "    self.input_dim: int = input_dim\n",
    "    self.output_dim: int = output_dim\n",
    "\n",
    "    self.feature_layer = nn.Sequential(\n",
    "      nn.Linear(input_dim, 128),\n",
    "      nn.LayerNorm(128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128),\n",
    "      nn.LayerNorm(128),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.value_stream = nn.Sequential(\n",
    "      nn.Linear(128, 128),\n",
    "      nn.LayerNorm(128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 1)\n",
    "    )\n",
    "\n",
    "    self.advantage_stream = nn.Sequential(\n",
    "      nn.Linear(128, 128),\n",
    "      nn.LayerNorm(128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, self.output_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.feature_layer(x)\n",
    "    value = self.value_stream(x)\n",
    "    adv = self.advantage_stream(x)\n",
    "    qvals = value + (adv - adv.mean())\n",
    "    return value, adv, qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision condition\n",
    "\n",
    "Class defines moments when model will make actions.  \n",
    "Model's reaction on __each__ snapshot update would be incredibly expensive, thus it reacts _volume-based_.  \n",
    "\n",
    "## Agent\n",
    "\n",
    "Class stores inner operation for RL:\n",
    "\n",
    "- Stores _replay buffer_  \n",
    "- Generates actions via model or random  \n",
    "- Evaluates rewards and updates models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', 'prev_obs prev_ps action obs ps meta done')\n",
    "\n",
    "class DecisionCondition:\n",
    "  def __init__(self, volume: float):\n",
    "    self.volume_condition: float = volume\n",
    "    self.reset()\n",
    "\n",
    "  def __call__(self, event: Union[Trade, OrderBook]):\n",
    "    if isinstance(event, Trade) and event.symbol == 'XBTUSD':\n",
    "      self.volume += event.volume\n",
    "      if self.volume > self.volume_condition:\n",
    "        self.volume %= self.volume_condition\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def reset(self):\n",
    "    self.volume = 0.0\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, model: DuelingDQN, target: DuelingDQN,\n",
    "               condition: DecisionCondition,\n",
    "               gamma = 0.99, lr=1e-3,\n",
    "               update_each:int=4,\n",
    "               buffer_size:int=30000, batch_size=1024):\n",
    "    self._replay_buffer: Deque[State] = deque(maxlen=buffer_size)\n",
    "    self._batch_size: int = batch_size\n",
    "    self.condition: DecisionCondition = condition\n",
    "    self._model : DuelingDQN = model\n",
    "    self._target: DuelingDQN = target\n",
    "    self.end_episode_states: List = []\n",
    "    self.episode_files: List[Tuple[str, str]] = []\n",
    "\n",
    "    self.EPS_START = 0.9\n",
    "    self.EPS_END = 0.1\n",
    "    self.EPS_DECAY = 400\n",
    "\n",
    "\n",
    "    self.gamma = gamma\n",
    "    self.MSE_loss = torch.nn.MSELoss()\n",
    "    self.optimizer = torch.optim.RMSprop(self._model.parameters(), lr=lr)\n",
    "\n",
    "    self._update_each = update_each\n",
    "    self.episode_counter = 0\n",
    "    self.reset_state()\n",
    "\n",
    "  def episode_results(self) -> pd.DataFrame:\n",
    "    fnames, states = agent.episode_files, agent.end_episode_states\n",
    "    states = [t[0].tolist() + [t[1]] for t in states]\n",
    "    episodes = [list(t[0]) + t[1] for t in zip(fnames, states)]\n",
    "    res = pd.DataFrame(episodes, columns=['ob_file', 'tr_file', 'usd', 'xbt', 'eth', 'xbt_price'])\n",
    "    return res\n",
    "\n",
    "  def reset_state(self):\n",
    "    self.obs = None\n",
    "    self.action = None\n",
    "    self.ps = None\n",
    "    self.episode_counter += 1\n",
    "    self.condition.reset()\n",
    "\n",
    "    # reload weights\n",
    "    if (self.episode_counter + 1) % self._update_each == 0:\n",
    "      self._target.load_state_dict(self._model.state_dict())\n",
    "\n",
    "  def get_reward(self, prev_v: torch.Tensor, v: torch.Tensor,\n",
    "                 prev_ps: torch.Tensor, ps: torch.Tensor,\n",
    "                 tau: torch.Tensor, a=1.5, b=1./1000):\n",
    "    vv = a * (v - prev_v)\n",
    "    vv.squeeze_(1)\n",
    "\n",
    "    state_delta = torch.abs(prev_ps) - torch.abs(ps) # todo: updated here, react negatively on accumulation of assets\n",
    "    pos = (torch.exp(b*tau) * state_delta) # todo: updated here, instead of sign, use delta\n",
    "\n",
    "    # a(V_t - V_{t-1}) + e^{b*tau} * sgn(|i_t| - |i_{t-1}|)\n",
    "    return vv + pos\n",
    "\n",
    "  def get_terminal_reward(self, terminal_ps, terminal_prices, alpha=3.0):\n",
    "    return alpha - torch.exp(-(terminal_ps[:, 0] / terminal_prices - terminal_ps[:, 1])) # 0: usd, 1: xbtusd, 2: ethusd\n",
    "\n",
    "  def store_episode(self, new_obs, new_ps, meta, done, action):\n",
    "    if self.is_initialized():\n",
    "      self._replay_buffer.append((self.obs, self.ps, self.action, new_obs, new_ps, meta, done))\n",
    "    self.obs = new_obs\n",
    "    self.ps = new_ps\n",
    "    self.action = action\n",
    "\n",
    "  def is_initialized(self):\n",
    "    return self.obs is not None  # happens after reset, first obs is missing\n",
    "\n",
    "  def get_action(self, obs, ps, eps=0.8): # todo: epsilon must change accordging to curve\n",
    "    eps = self.EPS_END + (self.EPS_START - self.EPS_END) * np.exp(-1. * self.episode_counter / self.EPS_DECAY)\n",
    "    if np.random.uniform(0.0, 1.0) < eps:\n",
    "      return random.randint(0, self._model.output_dim - 1)\n",
    "    with torch.no_grad():\n",
    "      _, _, qvals = self._target(torch.tensor(obs, dtype=torch.float).unsqueeze(0))\n",
    "      action = torch.argmax(qvals).cpu().detach().item()\n",
    "      return action\n",
    "\n",
    "  def update(self):\n",
    "    if len(self._replay_buffer) > self._batch_size:\n",
    "      items = random.sample(self._replay_buffer, self._batch_size)\n",
    "      prev_obs, prev_ps, action, obs, ps, meta, done = zip(*items)\n",
    "      prev_obs, prev_ps, obs, ps, meta = map(lambda x: torch.tensor(x, dtype=torch.float), [prev_obs, prev_ps, obs, ps, meta])\n",
    "      done = torch.tensor(done, dtype=torch.bool)\n",
    "      action = torch.tensor(action, dtype=torch.long).unsqueeze(1)\n",
    "\n",
    "      v, _, qvalues = self._model(prev_obs) # todo: here are target and current models?\n",
    "      next_v, _, next_qvalues = self._target(obs)\n",
    "\n",
    "      rewards = torch.empty_like(done, dtype=torch.float)\n",
    "      rewards[done] = self.get_terminal_reward(ps[done], meta[done][:, 0])\n",
    "      rewards[~done] = self.get_reward(v[~done], next_v[~done], prev_ps[~done][:, 1], ps[~done][:, 1], meta[~done][:, 1])\n",
    "\n",
    "      qvalues = qvalues.gather(1, action).squeeze(1)\n",
    "      next_qvalues = next_qvalues.max(1)[0]\n",
    "      expected_q = rewards + self.gamma * next_qvalues\n",
    "\n",
    "      loss = self.MSE_loss(qvalues, expected_q)  # or hubert loss\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      for param in self._model.parameters():\n",
    "        param.grad.data.clamp_(-1., 1.)\n",
    "      self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Strategy\n",
    "\n",
    "Class wrapper for RL operations: \n",
    "\n",
    "- Provides observations, portfolio states\n",
    "- Transforms action space into orders\n",
    "- Communicates with backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLStrategy(Strategy):\n",
    "  def __init__(self, agent: Agent, simulation_end: datetime.datetime, **kwags):\n",
    "    super().__init__(**kwags)\n",
    "    self.agent: Agent = agent\n",
    "    self._simulation_end = simulation_end\n",
    "    self.action_space: Dict[int, Tuple[int, int]] = {\n",
    "      0: (0, 0),\n",
    "      1: (1, 0),\n",
    "      2: (2, 0),\n",
    "      3: (0, 1),\n",
    "      4: (1, 1),\n",
    "      5: (1, 2),\n",
    "      6: (2, 0),\n",
    "      7: (2, 1),\n",
    "      8: (2, 2)\n",
    "    }\n",
    "\n",
    "  def return_unfinished(self, statuses: List[OrderStatus], memory: Dict[str, Union[Trade, OrderBook]]):\n",
    "    obs, ps, meta = self.get_observation(), self.get_state(), (self.get_prices(memory), 0.0)\n",
    "    self.agent.store_episode(obs, ps, meta, True, None)\n",
    "    self.agent.end_episode_states.append((ps, meta[0])) # end state and prices\n",
    "    # self.agent.update() # todo: remove this line later on\n",
    "    self.agent.reset_state()\n",
    "\n",
    "    super().return_unfinished(statuses, memory)\n",
    "\n",
    "  def get_observation(self):\n",
    "    # transform trivial metrics\n",
    "    items = list(map(lambda name: self.metrics_map[name].to_numpy(), names))\n",
    "\n",
    "    # transformation for time metrics\n",
    "    # items += [np.array(list(map(lambda x: list(x.values()), self.metrics_map[name].latest.values()))) for name in time_names]\n",
    "    items += list(map(lambda name: self.metrics_map[name].to_numpy(), time_names))\n",
    "    items = [t.flatten() for t in items]\n",
    "    items = np.concatenate(items, axis=None)\n",
    "    if len(items) != 38:\n",
    "      print('here')\n",
    "    return items\n",
    "    # return items\n",
    "\n",
    "  def get_prices(self, memory) -> float: # todo: refactor and use vwap\n",
    "    xbt: OrderBook = memory[('orderbook', 'XBTUSD')]\n",
    "    # eth: OrderBook = memory[('orderbook', 'ETHUSD')]\n",
    "\n",
    "    xbt_midprice = (xbt.ask_prices[0] + xbt.bid_prices[0]) / 2\n",
    "    # eth_midprice = (eth.ask_prices[0] + eth.bid_prices[0]) / 2\n",
    "\n",
    "    # return eth_midprice / xbt_midprice\n",
    "    return xbt_midprice\n",
    "\n",
    "  def get_state(self) -> np.array:\n",
    "    return np.array(list(self.balance.values()))\n",
    "\n",
    "  def get_timeleft(self, ts: datetime.datetime) -> float:\n",
    "    return (self._simulation_end - ts).total_seconds()\n",
    "\n",
    "  def action_to_order(self, action: int, memory, ts, quantity: int) -> List[OrderRequest]: # value from 0 to 8\n",
    "    offset_bid, offset_ask = self.action_space[action]\n",
    "\n",
    "    offset_bid *= 0.5 # price step is .5 dollars\n",
    "    offset_ask *= 0.5\n",
    "\n",
    "    ob: OrderBook = memory[('orderbook', 'XBTUSD')]\n",
    "\n",
    "    return [OrderRequest.create_bid(ob.bid_prices[0] - offset_bid, quantity, 'XBTUSD', ts),\n",
    "            OrderRequest.create_ask(ob.ask_prices[0] + offset_ask, quantity, 'XBTUSD', ts)]\n",
    "\n",
    "  def define_orders(self, row: Union[Trade, OrderBook],\n",
    "                    statuses: List[OrderStatus],\n",
    "                    memory: Dict[str, Union[Trade, OrderBook]]) -> List[OrderRequest]:\n",
    "    if self.agent.condition(row):\n",
    "      obs = self.get_observation()\n",
    "      ps = self.get_state()\n",
    "      action = self.agent.get_action(obs, ps)\n",
    "      meta = (self.get_prices(memory), self.get_timeleft(row.timestamp))\n",
    "\n",
    "      self.agent.store_episode(obs, ps, meta, False, action)\n",
    "      self.agent.update()\n",
    "\n",
    "      orders = self.action_to_order(action, memory, row.timestamp, 1000)\n",
    "    else:\n",
    "      orders = []\n",
    "    return orders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrices: [2,3,2], [2,3,2], [2, 2], [2]\n",
    "names = ['vwap', 'liquidity-spectrum', 'hayashi-yoshido', 'lipton']\n",
    "# [2, 2], [2, 2]\n",
    "time_names = ['trade-metric-45', 'trade-metric-80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_simulation(agent: Agent, orderbook_file: str, trade_file: str):\n",
    "  vwap = VWAP_volume([int(2.5e5), int(1e6)], name='vwap', z_normalize=3000)\n",
    "  liq = LiquiditySpectrum(z_normalize=3000)\n",
    "\n",
    "  defaults = [\n",
    "    (('XBTUSD', 0), [0.0]),\n",
    "    (('XBTUSD', 1), [0.0]),\n",
    "    (('ETHUSD', 0), [0.0]),\n",
    "    (('ETHUSD', 1), [0.0]),\n",
    "  ]\n",
    "\n",
    "  trade_metric = TradeMetric(defaults, [\n",
    "    # ('quantity', lambda x: len(x)),\n",
    "    ('total', lambda trades: np.log(sum(map(lambda x: x.volume, trades))))\n",
    "  ], seconds=45) # todo: add z-normalize for time-metrics\n",
    "  trade_metric2 = TradeMetric(defaults, [\n",
    "    # ('quantity', lambda x: len(x)),\n",
    "    ('total', lambda trades: np.log(sum(map(lambda x: x.volume, trades))))\n",
    "  ], seconds=80)\n",
    "\n",
    "  hy = HayashiYoshido(seconds=90)\n",
    "  lipton = Lipton(hy.name)\n",
    "\n",
    "  # todo: refactor in backtesting auto-cancel queries with prices worse than top 3 levels\n",
    "  # todo: update reader to work with only `xbtusd`\n",
    "  reader = OrderbookReader(orderbook_file, trade_file, nrows=None, is_precomputed=True)\n",
    "  end_ts = reader.get_ending_moment()\n",
    "\n",
    "  strategy = RLStrategy(agent, simulation_end=end_ts, instant_metrics=[vwap, liq], delta_metrics=[hy],\n",
    "                        time_metrics_trade=[trade_metric, trade_metric2], composite_metrics=[lipton], initial_balance=0.0)\n",
    "  backtest = BacktestOnSample(reader, strategy, delay=300, warmup=True, stale_depth=5)\n",
    "  backtest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = DecisionCondition(150000.0)\n",
    "model: DuelingDQN = DuelingDQN(input_dim=38, output_dim=9)\n",
    "target: DuelingDQN = DuelingDQN(input_dim=38, output_dim=9)\n",
    "target.load_state_dict(model.state_dict())\n",
    "\n",
    "agent = Agent(model, target, condition, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = 'notebooks/time-sampled'\n",
    "pairs = list(zip(glob.glob(f'{dst_dir}/orderbook_*'), glob.glob(f'{dst_dir}/trade_*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(4):\n",
    "    ob_file, tr_file = random.choice(pairs)\n",
    "    init_simulation(agent, ob_file, tr_file)\n",
    "    agent.episode_files.append((ob_file, tr_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ob_file</th>\n",
       "      <th>tr_file</th>\n",
       "      <th>usd</th>\n",
       "      <th>xbt</th>\n",
       "      <th>eth</th>\n",
       "      <th>xbt_price</th>\n",
       "      <th>pnl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notebooks/time-sampled\\orderbook_2898.csv.gz</td>\n",
       "      <td>notebooks/time-sampled\\trade_2898.csv.gz</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>-0.224712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6580.75</td>\n",
       "      <td>7.227927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notebooks/time-sampled\\orderbook_2253.csv.gz</td>\n",
       "      <td>notebooks/time-sampled\\trade_2253.csv.gz</td>\n",
       "      <td>-5904.0</td>\n",
       "      <td>1.159256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5090.25</td>\n",
       "      <td>-3.099682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notebooks/time-sampled\\orderbook_3925.csv.gz</td>\n",
       "      <td>notebooks/time-sampled\\trade_3925.csv.gz</td>\n",
       "      <td>-10084.0</td>\n",
       "      <td>1.528370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6613.25</td>\n",
       "      <td>23.492262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>notebooks/time-sampled\\orderbook_177.csv.gz</td>\n",
       "      <td>notebooks/time-sampled\\trade_177.csv.gz</td>\n",
       "      <td>21468.0</td>\n",
       "      <td>-2.741321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7834.75</td>\n",
       "      <td>-9.565847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ob_file  \\\n",
       "0  notebooks/time-sampled\\orderbook_2898.csv.gz   \n",
       "1  notebooks/time-sampled\\orderbook_2253.csv.gz   \n",
       "2  notebooks/time-sampled\\orderbook_3925.csv.gz   \n",
       "3   notebooks/time-sampled\\orderbook_177.csv.gz   \n",
       "\n",
       "                                    tr_file      usd       xbt  eth  \\\n",
       "0  notebooks/time-sampled\\trade_2898.csv.gz   1486.0 -0.224712  0.0   \n",
       "1  notebooks/time-sampled\\trade_2253.csv.gz  -5904.0  1.159256  0.0   \n",
       "2  notebooks/time-sampled\\trade_3925.csv.gz -10084.0  1.528370  0.0   \n",
       "3   notebooks/time-sampled\\trade_177.csv.gz  21468.0 -2.741321  0.0   \n",
       "\n",
       "   xbt_price        pnl  \n",
       "0    6580.75   7.227927  \n",
       "1    5090.25  -3.099682  \n",
       "2    6613.25  23.492262  \n",
       "3    7834.75  -9.565847  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = agent.episode_results()\n",
    "res['pnl'] = res['usd'] + res['xbt'] * res['xbt_price']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
